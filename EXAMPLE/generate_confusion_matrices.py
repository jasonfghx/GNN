import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def generate_sample_confusion_matrices():
    """ç”Ÿæˆä¸‰ä¸ªæ¨¡å‹çš„ç¤ºä¾‹æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾"""
    print("=== ç”Ÿæˆä¸‰ä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯” ===")
    
    # æ¨¡æ‹Ÿä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    # å‡è®¾æµ‹è¯•é›†æœ‰1000ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­600ä¸ªæ´»æ€§ï¼Œ400ä¸ªéæ´»æ€§
    np.random.seed(42)
    
    # çœŸå®æ ‡ç­¾ (0: éæ´»æ€§, 1: æ´»æ€§)
    y_true = np.array([0] * 400 + [1] * 600)
    
    # æ¨¡æ‹Ÿä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    models_data = {
        'Concat': {
            'y_pred': np.array([0] * 350 + [1] * 50 + [0] * 80 + [1] * 520),  # è¾ƒä½æ€§èƒ½
            'accuracy': 0.870,
            'f1': 0.885
        },
        'Cross Attention': {
            'y_pred': np.array([0] * 370 + [1] * 30 + [0] * 60 + [1] * 540),  # ä¸­ç­‰æ€§èƒ½
            'accuracy': 0.910,
            'f1': 0.923
        },
        'Joint Graph': {
            'y_pred': np.array([0] * 380 + [1] * 20 + [0] * 40 + [1] * 560),  # æœ€ä½³æ€§èƒ½
            'accuracy': 0.940,
            'f1': 0.952
        }
    }
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('ä¸‰ç§èåˆç­–ç•¥çš„æ··æ·†çŸ©é˜µå¯¹æ¯”\nConfusion Matrix Comparison of Three Fusion Strategies', 
                 fontsize=16, fontweight='bold', y=1.02)
    
    results_summary = []
    
    for idx, (model_name, data) in enumerate(models_data.items()):
        print(f"\nå¤„ç† {model_name} æ¨¡å‹...")
        
        y_pred = data['y_pred']
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        print(f"{model_name} æ··æ·†çŸ©é˜µ:\n{cm}")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        results_summary.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1
        })
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        ax = axes[idx]
        
        # ä½¿ç”¨seabornç»˜åˆ¶çƒ­å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['0', '1'], 
                   yticklabels=['0', '1'],
                   ax=ax, cbar_kws={'shrink': 0.8})
        
        ax.set_title(f'{model_name}\nAcc: {accuracy:.3f}, F1: {f1:.3f}', 
                    fontweight='bold', fontsize=12)
        ax.set_xlabel('Predicted Label', fontweight='bold')
        ax.set_ylabel('True Label', fontweight='bold')
        
        # æ·»åŠ æ€§èƒ½æ–‡æœ¬
        textstr = f'TP: {tp}\nTN: {tn}\nFP: {fp}\nFN: {fn}'
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
        ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
               verticalalignment='top', bbox=props)
        
        print(f"{model_name} - å‡†ç¡®ç‡: {accuracy:.4f}, F1åˆ†æ•°: {f1:.4f}")
    
    plt.tight_layout()
    plt.savefig('confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')
    print("\næ··æ·†çŸ©é˜µå¯¹æ¯”å›¾å·²ä¿å­˜ä¸º 'confusion_matrices_comparison.png'")
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===")
    print("-" * 80)
    print(f"{'æ¨¡å‹':<15} {'å‡†ç¡®ç‡':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10}")
    print("-" * 80)
    
    for result in results_summary:
        print(f"{result['Model']:<15} {result['Accuracy']:<10.4f} {result['Precision']:<10.4f} "
              f"{result['Recall']:<10.4f} {result['F1-Score']:<10.4f}")
    
    print("-" * 80)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = max(results_summary, key=lambda x: x['F1-Score'])
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']} (F1åˆ†æ•°: {best_model['F1-Score']:.4f})")
    
    return results_summary

def generate_detailed_performance_comparison():
    """ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”å›¾"""
    print("\n=== ç”Ÿæˆè¯¦ç»†æ€§èƒ½å¯¹æ¯”å›¾ ===")
    
    # æ€§èƒ½æ•°æ®
    models = ['Concat', 'Cross Attention', 'Joint Graph']
    metrics = {
        'Accuracy': [0.870, 0.910, 0.940],
        'Precision': [0.912, 0.947, 0.966],
        'Recall': [0.867, 0.900, 0.933],
        'F1-Score': [0.885, 0.923, 0.952]
    }
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
    fig, ax = plt.subplots(figsize=(12, 8))
    
    x = np.arange(len(models))
    width = 0.2
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    for i, (metric, values) in enumerate(metrics.items()):
        ax.bar(x + i * width, values, width, label=metric, color=colors[i], alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for j, v in enumerate(values):
            ax.text(x[j] + i * width, v + 0.01, f'{v:.3f}', 
                   ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    ax.set_xlabel('æ¨¡å‹ç±»å‹', fontweight='bold', fontsize=12)
    ax.set_ylabel('æ€§èƒ½æŒ‡æ ‡', fontweight='bold', fontsize=12)
    ax.set_title('ä¸‰ç§èåˆç­–ç•¥çš„æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\nPerformance Metrics Comparison of Three Fusion Strategies', 
                fontweight='bold', fontsize=14)
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(models)
    ax.legend(loc='upper left')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 1.1)
    
    plt.tight_layout()
    plt.savefig('performance_metrics_comparison.png', dpi=300, bbox_inches='tight')
    print("æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º 'performance_metrics_comparison.png'")
    plt.show()

if __name__ == "__main__":
    try:
        print("å¼€å§‹ç”Ÿæˆæ··æ·†çŸ©é˜µå¯¹æ¯”...")
        
        # ç”Ÿæˆæ··æ·†çŸ©é˜µå¯¹æ¯”
        results = generate_sample_confusion_matrices()
        
        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
        generate_detailed_performance_comparison()
        
        print("\nâœ… æ‰€æœ‰å›¾è¡¨ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - confusion_matrices_comparison.png (æ··æ·†çŸ©é˜µå¯¹æ¯”)")
        print("   - performance_metrics_comparison.png (æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”)")
        
        print("\nğŸ“ˆ ç»“æœåˆ†æ:")
        print("   - Joint Graph æ¨¡å‹è¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.952")
        print("   - Cross Attention æ¨¡å‹è¡¨ç°ä¸­ç­‰ï¼ŒF1åˆ†æ•°ä¸º0.923")
        print("   - Concat æ¨¡å‹è¡¨ç°ç›¸å¯¹è¾ƒä½ï¼ŒF1åˆ†æ•°ä¸º0.885")
        print("   - æ‰€æœ‰æ¨¡å‹éƒ½æ˜¾ç¤ºå‡ºè‰¯å¥½çš„é¢„æµ‹èƒ½åŠ›ï¼Œå‡†ç¡®ç‡å‡è¶…è¿‡87%")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()