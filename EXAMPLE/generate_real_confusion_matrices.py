import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch_geometric.data import Batch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥å¿…è¦çš„ç±»
try:
    from dengue_ns5_inhibitor_prediction import (
        MolecularGraphDataset, 
        ProteinGraphProcessor, 
        DengueNS5InhibitorPredictor,
        load_and_preprocess_data
    )
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€éœ€æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ dengue_ns5_inhibitor_prediction.py æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­")
    exit(1)

def load_trained_model(model_path, fusion_strategy):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    try:
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = DengueNS5InhibitorPredictor(fusion_strategy=fusion_strategy)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        checkpoint = torch.load(model_path, map_location='cpu')
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"  ä»checkpointåŠ è½½æ¨¡å‹çŠ¶æ€")
        else:
            model.load_state_dict(checkpoint)
            print(f"  ç›´æ¥åŠ è½½æ¨¡å‹çŠ¶æ€")
        
        model.eval()
        print(f"âœ… æ¨¡å‹ {model_path} åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")
        return None

def evaluate_model_predictions(model, test_loader, protein_data, strategy_name):
    """è¯„ä¼°æ¨¡å‹å¹¶è·å–é¢„æµ‹ç»“æœ"""
    print(f"æ­£åœ¨è¯„ä¼° {strategy_name} æ¨¡å‹...")
    model.eval()
    all_predictions = []
    all_true_labels = []
    
    with torch.no_grad():
        for batch_idx, (batch_mol, batch_labels) in enumerate(test_loader):
            try:
                outputs = model(batch_mol, protein_data)
                predictions = (outputs > 0.5).float()
                
                all_predictions.extend(predictions.cpu().numpy())
                all_true_labels.extend(batch_labels.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{len(test_loader)}")
                    
            except Exception as e:
                print(f"  æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                continue
    
    print(f"âœ… {strategy_name} æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œå…±å¤„ç† {len(all_predictions)} ä¸ªæ ·æœ¬")
    return np.array(all_true_labels), np.array(all_predictions)

def generate_real_confusion_matrices():
    """ç”ŸæˆåŸºäºçœŸå®æ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾"""
    print("=== ç”ŸæˆåŸºäºçœŸå®æ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯” ===")
    
    try:
        # 1. åŠ è½½æ•°æ®
        print("\n1. åŠ è½½æ•°æ®...")
        smiles_list, labels = load_and_preprocess_data()
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(smiles_list)} ä¸ªåŒ–åˆç‰©")
        print(f"   æ´»æ€§åŒ–åˆç‰©: {sum(labels)} ä¸ª")
        print(f"   éæ´»æ€§åŒ–åˆç‰©: {len(labels) - sum(labels)} ä¸ª")
        
        # 2. å¤„ç†è›‹ç™½è´¨ç»“æ„
        print("\n2. å¤„ç†è›‹ç™½è´¨ç»“æ„...")
        protein_processor = ProteinGraphProcessor('Dengue virus 3åºåˆ—.pdb')
        protein_data = protein_processor.protein_graph
        print("âœ… è›‹ç™½è´¨ç»“æ„å¤„ç†å®Œæˆ")
        
        # 3. åˆ›å»ºæ•°æ®é›†
        print("\n3. åˆ›å»ºæ•°æ®é›†...")
        dataset = MolecularGraphDataset(smiles_list, labels)
        print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
        
        # 4. æ•°æ®åˆ†å‰²ï¼ˆä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç¡®ä¿ä¸€è‡´æ€§ï¼‰
        print("\n4. æ•°æ®åˆ†å‰²...")
        train_indices, test_indices = train_test_split(
            range(len(dataset)), test_size=0.2, random_state=42, stratify=labels
        )
        print(f"âœ… è®­ç»ƒé›†: {len(train_indices)} ä¸ªæ ·æœ¬ï¼Œæµ‹è¯•é›†: {len(test_indices)} ä¸ªæ ·æœ¬")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        def collate_fn(batch):
            mol_graphs, labels = zip(*batch)
            return Batch.from_data_list(mol_graphs), torch.tensor(labels, dtype=torch.float)
        
        test_dataset = torch.utils.data.Subset(dataset, test_indices)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œå…± {len(test_loader)} ä¸ªæ‰¹æ¬¡")
        
        # 5. æ¨¡å‹é…ç½®
        model_configs = [
            {'path': 'best_dengue_ns5_model_concat.pth', 'strategy': 'concat', 'name': 'Concat'},
            {'path': 'best_dengue_ns5_model_cross_attention.pth', 'strategy': 'cross_attention', 'name': 'Cross Attention'},
            {'path': 'best_dengue_ns5_model_joint_graph.pth', 'strategy': 'joint_graph', 'name': 'Joint Graph'}
        ]
        
        # 6. åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        fig.suptitle('ä¸‰ç§èåˆç­–ç•¥çš„çœŸå®æ··æ·†çŸ©é˜µå¯¹æ¯”\nReal Confusion Matrix Comparison of Three Fusion Strategies', 
                     fontsize=16, fontweight='bold', y=1.02)
        
        results_summary = []
        successful_models = 0
        
        for idx, config in enumerate(model_configs):
            print(f"\n5.{idx+1} å¤„ç† {config['name']} æ¨¡å‹...")
            
            # åŠ è½½æ¨¡å‹
            model = load_trained_model(config['path'], config['strategy'])
            if model is None:
                print(f"âš ï¸ è·³è¿‡ {config['name']} æ¨¡å‹")
                # åˆ›å»ºç©ºçš„æ··æ·†çŸ©é˜µæ˜¾ç¤º
                ax = axes[idx]
                ax.text(0.5, 0.5, f'{config["name"]}\næ¨¡å‹åŠ è½½å¤±è´¥', 
                       ha='center', va='center', transform=ax.transAxes, fontsize=14)
                ax.set_title(f'{config["name"]} (æ¨¡å‹æœªæ‰¾åˆ°)', fontweight='bold', fontsize=12)
                ax.set_xlabel('Predicted Label', fontweight='bold')
                ax.set_ylabel('True Label', fontweight='bold')
                continue
            
            # è·å–é¢„æµ‹ç»“æœ
            y_true, y_pred = evaluate_model_predictions(model, test_loader, protein_data, config['name'])
            
            if len(y_true) == 0:
                print(f"âŒ {config['name']} æ¨¡å‹é¢„æµ‹å¤±è´¥")
                ax = axes[idx]
                ax.text(0.5, 0.5, f'{config["name"]}\né¢„æµ‹å¤±è´¥', 
                       ha='center', va='center', transform=ax.transAxes, fontsize=14)
                ax.set_title(f'{config["name"]} (é¢„æµ‹å¤±è´¥)', fontweight='bold', fontsize=12)
                continue
            
            successful_models += 1
            
            # è®¡ç®—æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)
            print(f"âœ… {config['name']} æ··æ·†çŸ©é˜µ:\n{cm}")
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            tn, fp, fn, tp = cm.ravel()
            accuracy = (tp + tn) / (tp + tn + fp + fn)
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            results_summary.append({
                'Model': config['name'],
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1,
                'TP': tp,
                'TN': tn,
                'FP': fp,
                'FN': fn
            })
            
            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            ax = axes[idx]
            
            # ä½¿ç”¨seabornç»˜åˆ¶çƒ­å›¾
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['0', '1'], 
                       yticklabels=['0', '1'],
                       ax=ax, cbar_kws={'shrink': 0.8})
            
            ax.set_title(f'{config["name"]}\nAcc: {accuracy:.3f}, F1: {f1:.3f}', 
                        fontweight='bold', fontsize=12)
            ax.set_xlabel('Predicted Label', fontweight='bold')
            ax.set_ylabel('True Label', fontweight='bold')
            
            # æ·»åŠ æ€§èƒ½æ–‡æœ¬
            textstr = f'TP: {tp}\nTN: {tn}\nFP: {fp}\nFN: {fn}'
            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
                   verticalalignment='top', bbox=props)
            
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f}, F1åˆ†æ•°: {f1:.4f}")
        
        plt.tight_layout()
        plt.savefig('real_confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')
        print(f"\nâœ… çœŸå®æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾å·²ä¿å­˜ä¸º 'real_confusion_matrices_comparison.png'")
        plt.close()
        
        # 7. æ‰“å°è¯¦ç»†ç»“æœ
        if results_summary:
            print("\n=== çœŸå®æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===")
            print("-" * 90)
            print(f"{'æ¨¡å‹':<15} {'å‡†ç¡®ç‡':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'æ ·æœ¬æ•°':<10}")
            print("-" * 90)
            
            for result in results_summary:
                sample_count = result['TP'] + result['TN'] + result['FP'] + result['FN']
                print(f"{result['Model']:<15} {result['Accuracy']:<10.4f} {result['Precision']:<10.4f} "
                      f"{result['Recall']:<10.4f} {result['F1-Score']:<10.4f} {sample_count:<10}")
            
            print("-" * 90)
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model = max(results_summary, key=lambda x: x['F1-Score'])
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']} (F1åˆ†æ•°: {best_model['F1-Score']:.4f})")
            
            # ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
            print("\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ===")
            for result in results_summary:
                print(f"\n{result['Model']} æ¨¡å‹:")
                print(f"  å‡†ç¡®ç‡: {result['Accuracy']:.4f}")
                print(f"  ç²¾ç¡®ç‡: {result['Precision']:.4f}")
                print(f"  å¬å›ç‡: {result['Recall']:.4f}")
                print(f"  F1åˆ†æ•°: {result['F1-Score']:.4f}")
                print(f"  çœŸé˜³æ€§: {result['TP']}, çœŸé˜´æ€§: {result['TN']}")
                print(f"  å‡é˜³æ€§: {result['FP']}, å‡é˜´æ€§: {result['FN']}")
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹")
        
        print(f"\nâœ… æˆåŠŸå¤„ç†äº† {successful_models}/3 ä¸ªæ¨¡å‹")
        return results_summary
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆçœŸå®æ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return []

if __name__ == "__main__":
    try:
        print("å¼€å§‹ç”ŸæˆåŸºäºçœŸå®æ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”...")
        results = generate_real_confusion_matrices()
        
        if results:
            print("\nğŸ‰ çœŸå®æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾ç”ŸæˆæˆåŠŸï¼")
            print("ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶: real_confusion_matrices_comparison.png")
        else:
            print("\nâš ï¸ çœŸå®æ··æ·†çŸ©é˜µç”Ÿæˆå¤±è´¥")
            print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä»¥åŠæ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()