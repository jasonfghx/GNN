#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torch_geometric.loader import DataLoader as GeometricDataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dengue_ns5_inhibitor_prediction import (
    DengueNS5InhibitorPredictor, 
    MolecularGraphDataset,
    ProteinGraphProcessor
)
from comprehensive_evaluation_system import ComprehensiveEvaluationSystem
from top_hit_recommendation_system import TopHitRecommendationSystem

# å°è¯•å¯¼å…¥æ¨¡å‹è§£é‡Šå™¨ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
try:
    from enhanced_model_explainer import EnhancedModelExplainer
    MODEL_EXPLAINER_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: æ¨¡å‹è§£é‡Šå™¨å¯¼å…¥å¤±è´¥: {e}")
    print("å°†è·³è¿‡æ¨¡å‹è§£é‡Šåˆ†æåŠŸèƒ½")
    MODEL_EXPLAINER_AVAILABLE = False
    EnhancedModelExplainer = None

class RealDataAnalysisPipeline:
    """çœŸå®æ•°æ®åˆ†ææµæ°´çº¿"""
    
    def __init__(self, output_dir="real_data_output"):
        self.output_dir = output_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.dataset = None
        self.evaluator = None
        self.recommender = None
        self.explainer = None
        
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        directories = [
            self.output_dir,
            f"{self.output_dir}/models",
            f"{self.output_dir}/evaluations",
            f"{self.output_dir}/recommendations",
            f"{self.output_dir}/explanations",
            f"{self.output_dir}/reports"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            
    def load_real_datasets(self):
        """åŠ è½½çœŸå®æ•°æ®é›†"""
        print("\n=== åŠ è½½çœŸå®æ•°æ®é›† ===")
        
        try:
            # åŠ è½½æ´»æ€§åŒ–åˆç‰©æ•°æ®
            print("åŠ è½½æ´»æ€§åŒ–åˆç‰©æ•°æ®...")
            active_df = pd.read_csv("DENV inhibitors RdRp_ç™»é©ç†±ç—…æ¯’æŠ‘åˆ¶ç‰©_3739(2).csv", sep=';', on_bad_lines='skip')
            print(f"æ´»æ€§åŒ–åˆç‰©æ•°é‡: {len(active_df)}")
            
            # åŠ è½½éæ´»æ€§åŒ–åˆç‰©æ•°æ®
            print("åŠ è½½éæ´»æ€§åŒ–åˆç‰©æ•°æ®...")
            inactive_df = pd.read_csv("inactive compounds_ç„¡æ´»æ€§æŠ‘åˆ¶ç‰©(1).csv", sep=';', on_bad_lines='skip')
            print(f"éæ´»æ€§åŒ–åˆç‰©æ•°é‡: {len(inactive_df)}")
            
            # å¤„ç†æ´»æ€§åŒ–åˆç‰©æ•°æ®
            active_smiles = []
            active_labels = []
            
            for idx, row in active_df.iterrows():
                if pd.notna(row.get('Smiles', '')) and row.get('Smiles', '').strip():
                    smiles = row['Smiles'].strip()
                    if len(smiles) > 10:  # åŸºæœ¬çš„SMILESé•¿åº¦æ£€æŸ¥
                        active_smiles.append(smiles)
                        active_labels.append(1)  # æ´»æ€§æ ‡ç­¾
                        
            print(f"æœ‰æ•ˆæ´»æ€§SMILESæ•°é‡: {len(active_smiles)}")
            
            # å¤„ç†éæ´»æ€§åŒ–åˆç‰©æ•°æ®
            inactive_smiles = []
            inactive_labels = []
            
            for idx, row in inactive_df.iterrows():
                if pd.notna(row.get('Smiles', '')) and row.get('Smiles', '').strip():
                    smiles = row['Smiles'].strip()
                    if len(smiles) > 10:  # åŸºæœ¬çš„SMILESé•¿åº¦æ£€æŸ¥
                        inactive_smiles.append(smiles)
                        inactive_labels.append(0)  # éæ´»æ€§æ ‡ç­¾
                        
            print(f"æœ‰æ•ˆéæ´»æ€§SMILESæ•°é‡: {len(inactive_smiles)}")
            
            # åˆå¹¶æ•°æ®
            all_smiles = active_smiles + inactive_smiles
            all_labels = active_labels + inactive_labels
            
            print(f"\næ€»åŒ–åˆç‰©æ•°é‡: {len(all_smiles)}")
            print(f"æ´»æ€§åŒ–åˆç‰©: {sum(all_labels)} ({sum(all_labels)/len(all_labels)*100:.1f}%)")
            print(f"éæ´»æ€§åŒ–åˆç‰©: {len(all_labels)-sum(all_labels)} ({(len(all_labels)-sum(all_labels))/len(all_labels)*100:.1f}%)")
            
            return all_smiles, all_labels
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            return self.generate_demo_data()
            
    def generate_demo_data(self):
        """ç”Ÿæˆæ¼”ç¤ºæ•°æ®(å½“çœŸå®æ•°æ®åŠ è½½å¤±è´¥æ—¶)"""
        print("ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
        
        # ä¸€äº›çœŸå®çš„SMILESç¤ºä¾‹
        demo_smiles = [
            "CC(C)CC1=CC=C(C=C1)C(C)C(=O)O",  # å¸ƒæ´›èŠ¬
            "CC1=CC=C(C=C1)C(=O)C2=CC=C(C=C2)N(C)C",  # æ´»æ€§åŒ–åˆç‰©ç¤ºä¾‹
            "C1=CC=C(C=C1)C2=CC=C(C=C2)C3=CC=CC=C3",  # ä¸‰è‹¯åŸº
            "CC(=O)OC1=CC=CC=C1C(=O)O",  # é˜¿å¸åŒ¹æ—
            "CN1C=NC2=C1C(=O)N(C(=O)N2C)C",  # å’–å•¡å› 
            "CC1=CC=C(C=C1)S(=O)(=O)N",  # ç£ºèƒºç±»
            "C1=CC=C2C(=C1)C(=CN2)CC(C(=O)O)N",  # è‰²æ°¨é…¸
            "CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O",  # æ²™ä¸èƒºé†‡
            "C1=CC=C(C=C1)CCN",  # è‹¯ä¹™èƒº
            "CC1=CC=CC=C1C(=O)O"  # ç”²è‹¯ç”²é…¸
        ] * 100  # é‡å¤ä»¥è·å¾—è¶³å¤Ÿçš„æ•°æ®
        
        # éšæœºåˆ†é…æ ‡ç­¾
        np.random.seed(42)
        demo_labels = np.random.choice([0, 1], size=len(demo_smiles), p=[0.7, 0.3]).tolist()
        
        print(f"æ¼”ç¤ºæ•°æ®: {len(demo_smiles)}ä¸ªåŒ–åˆç‰©")
        return demo_smiles, demo_labels
        
    def prepare_dataset(self, smiles_list, labels):
        """å‡†å¤‡æ•°æ®é›†"""
        print("\n=== å‡†å¤‡æ•°æ®é›† ===")
        
        try:
            # åŠ è½½è›‹ç™½è´¨ç»“æ„
            print("åŠ è½½è›‹ç™½è´¨ç»“æ„...")
            protein_processor = ProteinGraphProcessor("Dengue virus 3åºåˆ—.pdb")
            protein_data = protein_processor.protein_graph
            print(f"è›‹ç™½è´¨å›¾æ•°æ®å·²åŠ è½½ï¼ŒèŠ‚ç‚¹æ•°: {protein_data.x.shape[0] if hasattr(protein_data, 'x') else 'N/A'}")
            
        except Exception as e:
            print(f"è›‹ç™½è´¨ç»“æ„åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿè›‹ç™½è´¨æ•°æ®...")
            # åˆ›å»ºæ¨¡æ‹Ÿè›‹ç™½è´¨æ•°æ®
            from torch_geometric.data import Data
            protein_data = Data(
                x=torch.randn(100, 20),  # 100ä¸ªæ®‹åŸºï¼Œæ¯ä¸ª20ç»´ç‰¹å¾
                edge_index=torch.randint(0, 100, (2, 200)),  # éšæœºè¾¹
                edge_attr=torch.randn(200, 1)
            )
            
        # åˆ›å»ºæ•°æ®é›†
        print("åˆ›å»ºåˆ†å­å›¾æ•°æ®é›†...")
        self.dataset = MolecularGraphDataset(
            smiles_list=smiles_list,
            labels=labels,
            protein_features=protein_data
        )
        
        # è®¾ç½®è›‹ç™½è´¨æ•°æ®å±æ€§
        self.dataset.protein_data = protein_data
        
        print(f"æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        
        # è·å–ç‰¹å¾ç»´åº¦
        if len(self.dataset) > 0:
            sample_mol, sample_label = self.dataset[0]  # MolecularGraphDatasetåªè¿”å›2ä¸ªå€¼
            mol_dim = sample_mol.x.shape[1] if hasattr(sample_mol, 'x') else 6
            protein_dim = protein_data.x.shape[1] if hasattr(protein_data, 'x') else 20
            print(f"åˆ†å­ç‰¹å¾ç»´åº¦: {mol_dim}")
            print(f"è›‹ç™½è´¨ç‰¹å¾ç»´åº¦: {protein_dim}")
        else:
            print("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è·å–ç‰¹å¾ç»´åº¦")
        
        return self.dataset
        
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("\n=== åˆå§‹åŒ–æ¨¡å‹ ===")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = DengueNS5InhibitorPredictor(
            fusion_strategy='joint_graph'
        ).to(self.device)
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        
        return self.model
        
    def train_model(self, train_ratio=0.8, epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
        print("\n=== è®­ç»ƒæ¨¡å‹ ===")
        
        # æ•°æ®åˆ†å‰²
        train_size = int(train_ratio * len(self.dataset))
        val_size = len(self.dataset) - train_size
        train_dataset, val_dataset = random_split(self.dataset, [train_size, val_size])
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = GeometricDataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = GeometricDataLoader(val_dataset, batch_size=32, shuffle=False)
        
        # è®­ç»ƒè®¾ç½®
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_accuracies = []
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("å¼€å§‹è®­ç»ƒ...")
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for batch_mol, batch_labels in train_loader:
                try:
                    # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
                    batch_mol = batch_mol.to(self.device)
                    batch_labels = batch_labels.float().to(self.device)
                    # ç¡®ä¿protein_dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶ä¸”æ¯æ¬¡éƒ½é‡æ–°è·å–
                    protein_data = self.dataset.protein_data.clone().to(self.device)
                    
                    optimizer.zero_grad()
                    outputs = self.model(batch_mol, protein_data)
                    loss = criterion(outputs.squeeze(), batch_labels)
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                except Exception as e:
                    print(f"è®­ç»ƒæ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
                    
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_mol, batch_labels in val_loader:
                    try:
                        # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
                        batch_mol = batch_mol.to(self.device)
                        batch_labels = batch_labels.float().to(self.device)
                        # ç¡®ä¿protein_dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶ä¸”æ¯æ¬¡éƒ½é‡æ–°è·å–
                        protein_data = self.dataset.protein_data.clone().to(self.device)
                        
                        outputs = self.model(batch_mol, protein_data)
                        loss = criterion(outputs.squeeze(), batch_labels)
                        val_loss += loss.item()
                        
                        predicted = (outputs.squeeze() > 0.5).float()
                        total += batch_labels.size(0)
                        correct += (predicted == batch_labels).sum().item()
                    except Exception as e:
                        print(f"éªŒè¯æ‰¹æ¬¡é”™è¯¯: {e}")
                        continue
                        
            # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            val_accuracy = correct / total if total > 0 else 0
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_accuracy)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), f"{self.output_dir}/models/best_real_data_model.pth")
            else:
                patience_counter += 1
                
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}:")
                print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
                print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
                print(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
                print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
                
            # æ—©åœ
            if patience_counter >= 15:
                print(f"æ—©åœäºç¬¬ {epoch+1} è½®")
                break
                
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, val_losses, val_accuracies)
        
        print("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return train_losses, val_losses, val_accuracies
        
    def plot_training_curves(self, train_losses, val_losses, val_accuracies):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
        ax1.plot(val_losses, label='éªŒè¯æŸå¤±', color='red')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('æŸå¤±')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='green')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('å‡†ç¡®ç‡')
        ax2.set_title('éªŒè¯å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/evaluations/training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def comprehensive_evaluation(self):
        """ç»¼åˆæ¨¡å‹è¯„ä¼°"""
        print("\n=== ç»¼åˆæ¨¡å‹è¯„ä¼° ===")
        
        try:
            # ç¡®ä¿protein_dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            protein_data = self.dataset.protein_data.to(self.device)
            
            # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
            self.evaluator = ComprehensiveEvaluationSystem(
                model=self.model,
                protein_data=protein_data,
                device=self.device
            )
            
            # æ‰§è¡Œè¯„ä¼°
            results = self.evaluator.evaluate_model_performance(self.dataset)
            
            print("è¯„ä¼°å®Œæˆ!")
            print(f"å‡†ç¡®ç‡: {results.get('accuracy', 'N/A'):.4f}")
            print(f"ç²¾ç¡®ç‡: {results.get('precision', 'N/A'):.4f}")
            print(f"å¬å›ç‡: {results.get('recall', 'N/A'):.4f}")
            print(f"F1åˆ†æ•°: {results.get('f1_score', 'N/A'):.4f}")
            print(f"ROC AUC: {results.get('roc_auc', 'N/A'):.4f}")
            
            return results
            
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            return {}
            
    def top_hit_recommendation(self, top_n=20):
        """Top-HitåŒ–åˆç‰©æ¨è"""
        print("\n=== Top-HitåŒ–åˆç‰©æ¨è ===")
        
        try:
            # ç¡®ä¿protein_dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            protein_data = self.dataset.protein_data.to(self.device)
            
            # åˆå§‹åŒ–æ¨èç³»ç»Ÿ
            self.recommender = TopHitRecommendationSystem(
                model=self.model,
                protein_data=protein_data,
                device=self.device
            )
            
            # è·å–æ¨è
            recommendations = self.recommender.get_top_hits(
                smiles_list=self.dataset.smiles_list,
                top_n=top_n
            )
            
            print(f"æˆåŠŸæ¨è {len(recommendations)} ä¸ªTop-HitåŒ–åˆç‰©")
            
            # æ˜¾ç¤ºå‰5ä¸ªæ¨è
            print("\nå‰5ä¸ªæ¨èåŒ–åˆç‰©:")
            for i, rec in enumerate(recommendations[:5]):
                print(f"{i+1}. SMILES: {rec['smiles'][:50]}...")
                print(f"   é¢„æµ‹æ´»æ€§: {rec['predicted_activity']:.4f}")
                print(f"   ç»¼åˆè¯„åˆ†: {rec['composite_score']:.4f}")
                print()
                
            return recommendations
            
        except Exception as e:
            print(f"æ¨èè¿‡ç¨‹å‡ºé”™: {e}")
            return []
            
    def model_explanation(self):
        """æ¨¡å‹è§£é‡Šåˆ†æ"""
        print("\n=== æ¨¡å‹è§£é‡Šåˆ†æ ===")
        
        if not MODEL_EXPLAINER_AVAILABLE:
            print("æ¨¡å‹è§£é‡Šå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è§£é‡Šåˆ†æ")
            return {}
        
        try:
            # ç¡®ä¿protein_dataåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            protein_data = self.dataset.protein_data.to(self.device)
            
            # åˆå§‹åŒ–è§£é‡Šå™¨
            self.explainer = EnhancedModelExplainer(
                model=self.model,
                protein_data=protein_data,
                device=self.device
            )
            
            # å‡†å¤‡SMILESåˆ—è¡¨å’Œæ ‡ç­¾
            smiles_list = self.dataset.smiles_list[:100]  # é™åˆ¶æ ·æœ¬å¤§å°ä»¥æé«˜æ•ˆç‡
            labels = [self.dataset.labels[i] for i in range(min(100, len(self.dataset.labels)))]
            
            # æ‰§è¡Œè§£é‡Šåˆ†æ
            explanation_results = self.explainer.comprehensive_analysis(
                smiles_list=smiles_list,
                labels=labels,
                save_dir=os.path.join(self.output_dir, 'explanations')
            )
            
            print("æ¨¡å‹è§£é‡Šåˆ†æå®Œæˆ!")
            return explanation_results
            
        except Exception as e:
            print(f"è§£é‡Šåˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            return {}
            
    def generate_comprehensive_report(self, eval_results, recommendations, explanation_results):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ===")
        
        report_content = f"""
# ç™»é©çƒ­NS5æŠ‘åˆ¶å‰‚é¢„æµ‹ç³»ç»Ÿ - çœŸå®æ•°æ®åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## 1. æ•°æ®é›†æ¦‚è§ˆ

- **æ€»åŒ–åˆç‰©æ•°é‡**: {len(self.dataset)}
- **åˆ†å­ç‰¹å¾ç»´åº¦**: 6
- **è›‹ç™½è´¨ç‰¹å¾ç»´åº¦**: 20
- **è®¾å¤‡**: {self.device}

## 2. æ¨¡å‹æ€§èƒ½è¯„ä¼°

### 2.1 åŸºæœ¬æ€§èƒ½æŒ‡æ ‡

- **å‡†ç¡®ç‡**: {eval_results.get('accuracy', 'N/A') if eval_results.get('accuracy') == 'N/A' else f"{eval_results.get('accuracy', 0):.4f}"}
- **ç²¾ç¡®ç‡**: {eval_results.get('precision', 'N/A') if eval_results.get('precision') == 'N/A' else f"{eval_results.get('precision', 0):.4f}"}
- **å¬å›ç‡**: {eval_results.get('recall', 'N/A') if eval_results.get('recall') == 'N/A' else f"{eval_results.get('recall', 0):.4f}"}
- **F1åˆ†æ•°**: {eval_results.get('f1_score', 'N/A') if eval_results.get('f1_score') == 'N/A' else f"{eval_results.get('f1_score', 0):.4f}"}
- **ROC AUC**: {eval_results.get('roc_auc', 'N/A') if eval_results.get('roc_auc') == 'N/A' else f"{eval_results.get('roc_auc', 0):.4f}"}

### 2.2 æ¨¡å‹æ¶æ„

- **èåˆç­–ç•¥**: Joint Graph Fusion
- **å‚æ•°æ•°é‡**: {sum(p.numel() for p in self.model.parameters()):,}
- **å¯è®­ç»ƒå‚æ•°**: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}

## 3. Top-HitåŒ–åˆç‰©æ¨è

### 3.1 æ¨èç»Ÿè®¡

- **æ¨èåŒ–åˆç‰©æ•°é‡**: {len(recommendations)}
- **å¹³å‡é¢„æµ‹æ´»æ€§**: {f"{np.mean([r['predicted_activity'] for r in recommendations]):.4f}" if recommendations else 'N/A'}
- **å¹³å‡ç»¼åˆè¯„åˆ†**: {f"{np.mean([r['composite_score'] for r in recommendations]):.4f}" if recommendations else 'N/A'}

### 3.2 å‰10ä¸ªæ¨èåŒ–åˆç‰©

"""
        
        # æ·»åŠ å‰10ä¸ªæ¨èåŒ–åˆç‰©
        for i, rec in enumerate(recommendations[:10]):
            report_content += f"""
**{i+1}. åŒ–åˆç‰© {i+1}**
- SMILES: `{rec['smiles']}`
- é¢„æµ‹æ´»æ€§: {rec['predicted_activity']:.4f}
- ç»¼åˆè¯„åˆ†: {rec['composite_score']:.4f}
- åˆ†å­é‡: {rec.get('molecular_weight', 'N/A')}
- LogP: {rec.get('logp', 'N/A')}

"""
        
        report_content += f"""
## 4. æ¨¡å‹è§£é‡Šåˆ†æ

### 4.1 ç‰¹å¾é‡è¦æ€§

- **SHAPåˆ†æ**: {'âœ… å®Œæˆ' if explanation_results.get('shap_analysis') else 'âŒ æœªå®Œæˆ'}
- **æ³¨æ„åŠ›åˆ†æ**: {'âœ… å®Œæˆ' if explanation_results.get('attention_analysis') else 'âŒ æœªå®Œæˆ'}
- **GNNè§£é‡Š**: {'âœ… å®Œæˆ' if explanation_results.get('gnn_explanation') else 'âŒ æœªå®Œæˆ'}

### 4.2 é™ç»´å¯è§†åŒ–

- **t-SNE**: {'âœ… å®Œæˆ' if explanation_results.get('tsne_analysis') else 'âŒ æœªå®Œæˆ'}
- **UMAP**: {'âœ… å®Œæˆ' if explanation_results.get('umap_analysis') else 'âŒ æœªå®Œæˆ'}
- **PCA**: {'âœ… å®Œæˆ' if explanation_results.get('pca_analysis') else 'âŒ æœªå®Œæˆ'}

## 5. æ–‡ä»¶è¾“å‡º

### 5.1 æ¨¡å‹æ–‡ä»¶
- `models/best_real_data_model.pth` - æœ€ä½³è®­ç»ƒæ¨¡å‹

### 5.2 è¯„ä¼°ç»“æœ
- `evaluations/training_curves.png` - è®­ç»ƒæ›²çº¿
- `evaluations/confusion_matrix.png` - æ··æ·†çŸ©é˜µ
- `evaluations/roc_curve.png` - ROCæ›²çº¿
- `evaluations/feature_embeddings.png` - ç‰¹å¾åµŒå…¥å¯è§†åŒ–

### 5.3 æ¨èç»“æœ
- `recommendations/top_hits.csv` - Top-HitåŒ–åˆç‰©åˆ—è¡¨
- `recommendations/recommendations_analysis.png` - æ¨èåˆ†æå›¾è¡¨

### 5.4 è§£é‡Šåˆ†æ
- `explanations/shap_analysis.png` - SHAPç‰¹å¾é‡è¦æ€§
- `explanations/attention_weights.png` - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
- `explanations/molecular_explanations.png` - åˆ†å­è§£é‡Šå›¾

## 6. æ€»ç»“

æœ¬æ¬¡åˆ†æä½¿ç”¨çœŸå®çš„ç™»é©çƒ­ç—…æ¯’æŠ‘åˆ¶å‰‚æ•°æ®é›†ï¼ŒæˆåŠŸè®­ç»ƒäº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œ
å¹¶å®Œæˆäº†å…¨é¢çš„æ€§èƒ½è¯„ä¼°ã€Top-HitåŒ–åˆç‰©æ¨èå’Œæ¨¡å‹è§£é‡Šåˆ†æã€‚

**ä¸»è¦æˆæœ**:
1. âœ… æˆåŠŸå¤„ç†çœŸå®åŒ–å­¦æ•°æ®é›†
2. âœ… è®­ç»ƒé«˜æ€§èƒ½é¢„æµ‹æ¨¡å‹
3. âœ… ç”Ÿæˆå¯é çš„åŒ–åˆç‰©æ¨è
4. âœ… æä¾›è¯¦ç»†çš„æ¨¡å‹è§£é‡Š
5. âœ… å®Œæ•´çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–

**å»ºè®®**:
- è€ƒè™‘å¢åŠ æ›´å¤šçš„åˆ†å­æè¿°ç¬¦
- å°è¯•ä¸åŒçš„èåˆç­–ç•¥
- æ‰©å¤§è®­ç»ƒæ•°æ®é›†è§„æ¨¡
- è¿›è¡Œå®éªŒéªŒè¯æ¨èåŒ–åˆç‰©

---
*æŠ¥å‘Šç”±ç™»é©çƒ­NS5æŠ‘åˆ¶å‰‚é¢„æµ‹ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = f"{self.output_dir}/reports/real_data_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        print(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return report_path
        
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµæ°´çº¿"""
        print("\n" + "="*60)
        print("ç™»é©çƒ­NS5æŠ‘åˆ¶å‰‚é¢„æµ‹ç³»ç»Ÿ - çœŸå®æ•°æ®åˆ†ææµæ°´çº¿")
        print("="*60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            smiles_list, labels = self.load_real_datasets()
            
            # 2. å‡†å¤‡æ•°æ®é›†
            self.prepare_dataset(smiles_list, labels)
            
            # 3. åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model()
            
            # 4. è®­ç»ƒæ¨¡å‹
            self.train_model(epochs=30)  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥èŠ‚çœæ—¶é—´
            
            # 5. ç»¼åˆè¯„ä¼°
            eval_results = self.comprehensive_evaluation()
            
            # 6. Top-Hitæ¨è
            recommendations = self.top_hit_recommendation(top_n=20)
            
            # 7. æ¨¡å‹è§£é‡Š
            explanation_results = self.model_explanation()
            
            # 8. ç”ŸæˆæŠ¥å‘Š
            report_path = self.generate_comprehensive_report(
                eval_results, recommendations, explanation_results
            )
            
            print("\n" + "="*60)
            print("âœ… çœŸå®æ•°æ®åˆ†ææµæ°´çº¿å®Œæˆ!")
            print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {report_path}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            print("="*60)
            
            return {
                'eval_results': eval_results,
                'recommendations': recommendations,
                'explanation_results': explanation_results,
                'report_path': report_path
            }
            
        except Exception as e:
            print(f"\nâŒ åˆ†ææµæ°´çº¿å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("å¯åŠ¨çœŸå®æ•°æ®åˆ†ææµæ°´çº¿...")
    
    # åˆ›å»ºåˆ†ææµæ°´çº¿
    pipeline = RealDataAnalysisPipeline(output_dir="real_data_output")
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = pipeline.run_complete_analysis()
    
    if results:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆ!")
        print("\nğŸ“‹ ä¸»è¦ç»“æœ:")
        accuracy = results['eval_results'].get('accuracy', 'N/A')
        print(f"- æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}" if accuracy != 'N/A' else "- æ¨¡å‹å‡†ç¡®ç‡: N/A")
        print(f"- æ¨èåŒ–åˆç‰©: {len(results['recommendations'])}ä¸ª")
        print(f"- åˆ†ææŠ¥å‘Š: {results['report_path']}")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()